{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gLM6o1DSjcXJ","executionInfo":{"status":"ok","timestamp":1735194789209,"user_tz":-330,"elapsed":399,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import datetime\n","import os\n","import os.path\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","from torchvision.utils import save_image\n","from torch.utils.data import SubsetRandomSampler\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","from IPython.display import clear_output\n","from tqdm import tqdm"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZ7QF4YbN1Pl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eeffc764-7316-45f2-c235-ef508317ec70","executionInfo":{"status":"ok","timestamp":1735194815455,"user_tz":-330,"elapsed":23752,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive\n"]}]},{"cell_type":"code","metadata":{"id":"5MdEuFt0nkmc","executionInfo":{"status":"ok","timestamp":1735194847021,"user_tz":-330,"elapsed":621,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","batch_size = 4\n","# Number of training epochs\n","num_epochs = 200\n","LOAD_MODEL = True\n","\n","#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/p_virus_200_2020-08-22_15:49:13.dat' #P_vir_200_opt\n","#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/p_bacteria_200_2020-08-22_16:21:47.dat' #P_bac_200_opt\n","#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/normal_200_2020-08-22_16:38:52.dat' #Normal_200_opt\n","#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/covid_200_2020-08-22_16:58:21.dat' #Covid_200_opt\n","\n","TRAIN_ALL = False\n","#All images will be resized to this size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 1\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 64\n","\n","# Size of feature maps in discriminator\n","ndf = 64\n","\n","# Learning rate for optimizers\n","lr = 0.002\n","lr_d = 0.0002\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","# Beta2 hyperparam for Adam optimizers\n","beta2 = 0.999\n","\n","real_label = 1.\n","fake_label = 0.\n","# Input to generator\n","fixed_noise = torch.randn(64, nz, 1, 1, device=device) #batch of 64\n","# Define Loss function\n","criterion = nn.BCELoss()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHXGFAtBHH4R","executionInfo":{"status":"ok","timestamp":1735194852962,"user_tz":-330,"elapsed":377,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4uvXCj5zzNa","executionInfo":{"status":"ok","timestamp":1735194860720,"user_tz":-330,"elapsed":397,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["# class Generator(nn.Module):\n","\n","#     def __init__(self):\n","#         super(Generator, self).__init__()\n","#         self._model = nn.Sequential(\n","#             # input is Z, going into a convolution\n","#             #i/p,o/p,kernel size,stride,padding\n","#             nn.ConvTranspose2d( nz, ngf * 16, 4, 1, 0, bias=False),\n","#             nn.BatchNorm2d(ngf * 16),\n","#             nn.ReLU(True),\n","#             # state size. (ngf*16) x 4 x 4\n","#             nn.ConvTranspose2d( ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n","#             nn.BatchNorm2d(ngf * 8),\n","#             nn.ReLU(True),\n","#             # state size. (ngf*8) x 8 x 8\n","#             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","#             nn.BatchNorm2d(ngf * 4),\n","#             nn.ReLU(True),\n","#             # state size. (ngf*4) x 16 x 16\n","#             nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","#             nn.BatchNorm2d(ngf * 2),\n","#             nn.ReLU(True),\n","#             # state size. (ngf*2) x 32 x 32\n","#             nn.ConvTranspose2d( ngf*2, nc, 4, 2, 1, bias=False),\n","#             nn.Tanh()\n","#             # state size. (nc) x 64 x 64\n","#         )\n","\n","#     def forward(self, input):\n","#         return self._model(input)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEb5BlY5GO4_","executionInfo":{"status":"ok","timestamp":1735194864288,"user_tz":-330,"elapsed":589,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["# class Discriminator(nn.Module):\n","\n","#     def __init__(self):\n","#         super(Discriminator, self).__init__()\n","#         self._model = nn.Sequential(\n","#             # input is (nc) x 64 x 64\n","#             nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),\n","#             nn.LeakyReLU(0.2, inplace=True),\n","#             # state size. (ndf*2) x 32 x 32\n","#             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","#             nn.BatchNorm2d(ndf * 4),\n","#             nn.LeakyReLU(0.2, inplace=True),\n","#             # state size. (ndf*4) x 16 x 16\n","#             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","#             nn.BatchNorm2d(ndf * 8),\n","#             nn.LeakyReLU(0.2, inplace=True),\n","#             # state size. (ndf*8) x 8 x 8\n","#             nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n","#             nn.BatchNorm2d(ndf * 16),\n","#             nn.LeakyReLU(0.2, inplace=True),\n","#             # state size. (ndf*16) x 4 x 4\n","#             nn.Conv2d(ndf * 16, 1, 4, 1, 0, bias=False),\n","#             nn.Sigmoid()\n","#         )\n","\n","#     def forward(self, input):\n","#         return self._model(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(p=0.5),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(p=0.5),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.Dropout(p=0.25),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVLWLMetCrGe","executionInfo":{"status":"ok","timestamp":1735194869887,"user_tz":-330,"elapsed":413,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def plot(name, train_epoch, values, path, save):\n","    clear_output(wait=True)\n","    plt.close('all')\n","    fig = plt.figure()\n","    fig = plt.ion()\n","    fig = plt.subplot(1, 1, 1)\n","    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n","    fig = plt.ylabel(name)\n","    fig = plt.xlabel('train_set')\n","    fig = plt.plot(values)\n","    fig = plt.grid()\n","    get_fig = plt.gcf()\n","    fig = plt.draw()  # draw the plot\n","    fig = plt.pause(1)  # show it for 1 second\n","    if save:\n","        now = datetime.datetime.now()\n","        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n","                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHGbk-t3imrL","executionInfo":{"status":"ok","timestamp":1735194875528,"user_tz":-330,"elapsed":382,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n","    now = datetime.datetime.now()\n","    g_losses = metrics['train.G_losses'][-1]\n","    d_losses = metrics['train.D_losses'][-1]\n","    name = \"%+.3f_%+.3f_%d_%s.dat\" % (g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n","    fname = os.path.join('.', 'augGAN/model', name)\n","    states = {\n","            'state_dict_generator': generator.state_dict(),\n","            'state_dict_discriminator': discriminator.state_dict(),\n","            'gen_optimizer': gen_optimizer.state_dict(),\n","            'dis_optimizer': dis_optimizer.state_dict(),\n","            'metrics': metrics,\n","            'train_epoch': num_epochs,\n","            'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n","    }\n","    torch.save(states, fname)\n","    path='augGAN/plots/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n","    try:\n","      os.mkdir(os.path.join('.', path))\n","    except Exception as error:\n","      print(error)\n","\n","    plot('G_losses', num_epochs, metrics['train.G_losses'], path, True)\n","    plot('D_losses', num_epochs, metrics['train.D_losses'], path, True)\n","    plot('D_x', num_epochs, metrics['train.D_x'], path, True)\n","    plot('D_G_z1', num_epochs, metrics['train.D_G_z1'], path, True)\n","    plot('D_G_z2', num_epochs, metrics['train.D_G_z2'], path, True)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"xn5d4tccMIyL","executionInfo":{"status":"ok","timestamp":1735194886108,"user_tz":-330,"elapsed":376,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["img_list = []\n","G_losses = []\n","D_losses = []\n","\n","def train_gan(generator, discriminator, gen_optimizer, dis_optimizer, train_loader, num_epochs, metrics):\n","        iters = 0\n","        print(\"GAN training started :D...\")\n","\n","        for epoch in range(num_epochs):\n","            print(\"Epoch %d\" %(epoch+1))\n","            # For each batch in the dataloader\n","            for i, data in enumerate(tqdm(train_loader, 0)):\n","                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","                ## Train with all-real batch\n","                discriminator.zero_grad()\n","                # Format batch\n","                b_real = data[0].to(device)\n","                b_size = b_real.size(0)\n","                label = torch.full((b_size,), real_label, device=device)\n","                # Forward pass real batch through D\n","                output = discriminator(b_real).view(-1)\n","                # Calculate loss on all-real batch\n","                errD_real = criterion(output, label)\n","                # Calculate gradients for D in backward pass\n","                errD_real.backward()\n","                D_x = output.mean().item()\n","                metrics['train.D_x'].append(D_x)\n","\n","                ## Train with all-fake batch\n","                # Generate batch of latent vectors\n","                noise = torch.randn(b_size, nz, 1, 1, device=device)\n","                # Generate fake image batch with G\n","                fake = generator(noise)\n","                label.fill_(fake_label)\n","                # Classify all fake batch with D\n","                output = discriminator(fake.detach()).view(-1)\n","                # Calculate D's loss on the all-fake batch\n","                errD_fake = criterion(output, label)\n","                # Calculate the gradients for this batch\n","                errD_fake.backward()\n","                D_G_z1 = output.mean().item()\n","                metrics['train.D_G_z1'].append(D_G_z1)\n","                # Add the gradients from the all-real and all-fake batches\n","                errD = errD_real + errD_fake\n","                # Update D\n","                dis_optimizer.step()\n","                # if i>0:\n","                #     if errD.item()>G_losses[i-1]:\n","                #         dis_optimizer.step()\n","                # else:\n","                #     dis_optimizer.step()\n","\n","                # (2) Update G network: maximize log(D(G(z)))\n","                generator.zero_grad()\n","                label.fill_(real_label)  # fake labels are real for generator cost\n","                # Since we just updated D, perform another forward pass of all-fake batch through D\n","                output = discriminator(fake).view(-1)\n","                # Calculate G's loss based on this output\n","                errG = criterion(output, label)\n","                # Calculate gradients for G\n","                errG.backward()\n","                D_G_z2 = output.mean().item()\n","                metrics['train.D_G_z2'].append(D_G_z2)\n","                # Update G\n","                gen_optimizer.step()\n","\n","                # Save Losses for plotting later\n","                G_losses.append(errG.item())\n","                D_losses.append(errD.item())\n","                metrics['train.G_losses'].append(errG.item())\n","                metrics['train.D_losses'].append(errD.item())\n","\n","                # Check how the generator is doing by saving G's output on fixed_noise\n","                if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n","                    with torch.no_grad():\n","                        fake = generator(fixed_noise).detach().cpu()\n","                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","\n","                iters += 1\n","            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n","                  % (epoch+1, num_epochs, i, len(train_loader),\n","                    errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","        save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gPHMkQQKJItb","executionInfo":{"status":"ok","timestamp":1735194889047,"user_tz":-330,"elapsed":413,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def get_indices(dataset, class_name, indices):\n","    j = 0\n","    for i in range(len(dataset.targets)):\n","        if dataset.targets[i] == class_name:\n","            indices.append(i)\n","            j += 1\n","    print(\"Total Samples of class\", class_name,\"found are\",j)\n","    return indices"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekCtxRmkLl5R","executionInfo":{"status":"ok","timestamp":1735194895225,"user_tz":-330,"elapsed":397,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def test1(generator, discriminator, num_epochs, metrics):\n","    print('Testing Block.........')\n","    now = datetime.datetime.now()\n","    g_losses = metrics['train.G_losses'][-1]\n","    d_losses = metrics['train.D_losses'][-1]\n","    path='augGAN/output_images'\n","    try:\n","      os.mkdir(os.path.join('.', path))\n","    except Exception as error:\n","      print(error)\n","\n","    test_img_list = []\n","    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n","    test_fake = generator(test_noise).detach().cpu()\n","    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n","    fig = plt.figure(figsize=(15,15))\n","    fig = plt.axis(\"off\")\n","    fig = plt.title(\"Fake Images\")\n","    fig = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n","    get_fig = plt.gcf()\n","    fig = plt.show()\n","    get_fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n","                    (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjQZEYmqSRyn","executionInfo":{"status":"ok","timestamp":1735194898184,"user_tz":-330,"elapsed":398,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def test2(generator, discriminator, num_epochs, metrics, loader):\n","    print('Testing Block.........')\n","    now = datetime.datetime.now()\n","    g_losses = metrics['train.G_losses'][-1]\n","    d_losses = metrics['train.D_losses'][-1]\n","    path='augGAN/output_images'\n","    try:\n","      os.mkdir(os.path.join('.', path))\n","    except Exception as error:\n","      print(error)\n","\n","    real_batch = next(iter(loader))\n","\n","    test_img_list = []\n","    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n","    test_fake = generator(test_noise).detach().cpu()\n","    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n","\n","    fig = plt.figure(figsize=(15,15))\n","    ax1 = plt.subplot(1,2,1)\n","    ax1 = plt.axis(\"off\")\n","    ax1 = plt.title(\"Real Images\")\n","    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n","\n","    ax2 = plt.subplot(1,2,2)\n","    ax2 = plt.axis(\"off\")\n","    ax2 = plt.title(\"Fake Images\")\n","    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n","    #ax2 = plt.show()\n","    fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n","                    (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncRL2jjtN-Ns","executionInfo":{"status":"ok","timestamp":1735194903867,"user_tz":-330,"elapsed":385,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["def test_fake(generator, discriminator, metrics, n_images, folname):\n","    now = datetime.datetime.now()\n","    g_losses = metrics['train.G_losses'][-1]\n","    d_losses = metrics['train.D_losses'][-1]\n","    #path='augGAN/output_images/%+.3f_%+.3f_%d_%s'% (g_losses, d_losses, n_images, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n","    path='main_folder/'+str(n_images)+'/'+folname\n","    try:\n","      os.mkdir(os.path.join('.', path))\n","    except Exception as error:\n","      print(error)\n","\n","    im_batch_size = 50\n","    #n_images=100\n","    for i_batch in range(0, n_images, im_batch_size):\n","        gen_z = torch.randn(im_batch_size, 100, 1, 1, device=device)\n","        gen_images = generator(gen_z)\n","        dis_result = discriminator(gen_images).view(-1)\n","        images = gen_images.to(\"cpu\").clone().detach()\n","        images = images.numpy().transpose(0, 2, 3, 1)\n","        for i_image in range(gen_images.size(0)):\n","            save_image(gen_images[i_image, :, :, :], os.path.join(path,\n","                        f'image_{i_batch+i_image:04d}.png'), normalize= True)\n","\n","    print('Testing Block.........')\n","    print('Discriminator_mean: ', dis_result.mean().item())\n","    #import shutil\n","    #shutil.make_archive('images', 'zip', './augGAN/output_images')"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWeXmdInu38z","colab":{"base_uri":"https://localhost:8080/","height":426},"outputId":"1b0cf998-89d5-4be9-cf8b-eb2dcb5357c7","executionInfo":{"status":"error","timestamp":1735195454856,"user_tz":-330,"elapsed":901,"user":{"displayName":"VINAY DAGAR","userId":"04519018048582024975"}}},"source":["for func in [\n","    lambda: os.mkdir(os.path.join('.', 'augGAN')),\n","    lambda: os.mkdir(os.path.join('.', 'augGAN/model')),\n","    lambda: os.mkdir(os.path.join('.', 'augGAN/plots')),\n","    lambda: os.mkdir(os.path.join('.', 'augGAN/output_images'))]:  # create directories\n","  try:\n","    func()\n","  except Exception as error:\n","    print(error)\n","    continue\n","\n","METRIC_FIELDS = [\n","    'train.D_x',\n","    'train.D_G_z1',\n","    'train.D_G_z2',\n","    'train.G_losses',\n","    'train.D_losses',\n","]\n","metrics = {field: list() for field in METRIC_FIELDS}\n","\n","if nc==1:\n","    mu = (0.5)\n","    sigma = (0.5)\n","    transform = transforms.Compose([#transforms.RandomHorizontalFlip(),\n","                                    transforms.Grayscale(num_output_channels=1),\n","                                    transforms.Resize((64,64)),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize(mu, sigma)])\n","elif nc==3:\n","    mu = (0.5,0.5,0.5)\n","    sigma = (0.5,0.5,0.5)\n","    #Originally authors used just scaling\n","    transform = transforms.Compose([#transforms.RandomHorizontalFlip(),\n","                                    transforms.Resize((64,64)),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize(mu, sigma)])\n","else:\n","    print(\"Tranformation not defined for this option\")\n","\n","data_dir = '/Thyroid Data DCGAN/DU'\n","\n","trainset0 = datasets.ImageFolder(os.path.join(\n","          data_dir, \"Train/\"), transform=transform)\n","trainset500 = datasets.ImageFolder(os.path.join(\n","          data_dir, \"Train_classic/500/\"), transform=transform)\n","trainset1000 = datasets.ImageFolder(os.path.join(\n","          data_dir, \"train_classic/1000/\"), transform=transform)\n","trainset2000 = datasets.ImageFolder(os.path.join(\n","          data_dir, \"train_classic/2000/\"), transform=transform)\n","listtrainset_no_aug = [trainset0]\n","listtrainset_classic = [trainset500,trainset1000]#,trainset2000]\n","listtrainset = listtrainset_no_aug + listtrainset_classic\n","train_set = torch.utils.data.ConcatDataset(listtrainset)\n","\n","# train_set = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform=transform)\n","\n","generator = Generator().to(device)\n","discriminator = Discriminator().to(device)\n","generator.apply(weights_init)\n","discriminator.apply(weights_init)\n","gen_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n","dis_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n","\n","if LOAD_MODEL:\n","    if torch.cuda.is_available():\n","      checkpoint = torch.load(PATH)\n","    else:\n","      checkpoint = torch.load(PATH, map_location=lambda storage, loc: storage)\n","\n","    generator.load_state_dict(checkpoint['state_dict_generator'])\n","    discriminator.load_state_dict(checkpoint['state_dict_discriminator'])\n","    gen_optimizer.load_state_dict(checkpoint['gen_optimizer'])\n","    dis_optimizer.load_state_dict(checkpoint['dis_optimizer'])\n","    metrics=checkpoint['metrics']\n","    num_epochs=checkpoint['train_epoch']\n","    date=checkpoint['date']\n","    generator.train(mode=False)\n","    discriminator.train(mode=False)\n","    print('GAN loaded for epochs: ', num_epochs)\n","    print(generator)\n","    print(discriminator)\n","    print(gen_optimizer)\n","    print(dis_optimizer)\n","    print(date)\n","    #test1(generator, discriminator, num_epochs, metrics)\n","else:\n","    if TRAIN_ALL:\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n","                                                  shuffle=True)\n","        train_gan(generator, discriminator, gen_optimizer, dis_optimizer, train_loader,\n","                  num_epochs, metrics)\n","        test2(generator, discriminator, num_epochs, metrics, train_loader)\n","    else:\n","        # idx = []\n","        # idx = get_indices(train_set, 4, idx) #second argument is 0 for covid; 1 for normal; 2 for pneumonia_bacteria; 3 for pneumonia_virus for x-ray dataset\n","\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n","                                                  shuffle=True)\n","        mask =[x[1]==0 for x in train_loader.dataset] #here is 0 for covid; 1 for normal; 2 for pneumonia_bacteria; 3 for pneumonia_virus for x-ray dataset\n","        idx= np.arange(len(train_loader.dataset))[mask]\n","\n","        print(\"Total samples now are \",len(idx))\n","        selected_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n","                                                      sampler = SubsetRandomSampler(idx))\n","        train_gan(generator, discriminator, gen_optimizer, dis_optimizer, selected_loader,\n","                  num_epochs, metrics)\n","        test2(generator, discriminator, num_epochs, metrics, selected_loader)\n","        test1(generator, discriminator, num_epochs, metrics)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 17] File exists: './augGAN'\n","[Errno 17] File exists: './augGAN/model'\n","[Errno 17] File exists: './augGAN/plots'\n","[Errno 17] File exists: './augGAN/output_images'\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/Thyroid Data DCGAN/DU/Train'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-25333c1c6a16>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Thyroid Data DCGAN/DU'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m trainset0 = datasets.ImageFolder(os.path.join(\n\u001b[0m\u001b[1;32m     43\u001b[0m           data_dir, \"Train\"), transform=transform)\n\u001b[1;32m     44\u001b[0m trainset500 = datasets.ImageFolder(os.path.join(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Thyroid Data DCGAN/DU/Train'"]}]},{"cell_type":"code","metadata":{"id":"A1Zz7NErSjEG","colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"5044b9ed-2c23-4fd8-9fa1-6f495beb3f17"},"source":["#Testing cell....to visualize\n","test_batch = 1 #No of images to be genertaed in stack\n","test_fake_id = 1 #To generate fake images or just view real images\n","LOAD_ID = 0 #Class of images in case test_fake_id is 0\n","\n","if test_fake_id:\n","  #check for fake image\n","  test_img_list = []\n","  test_noise = torch.randn(test_batch, nz, 1, 1, device=device)\n","  test_img = generator(test_noise)#.detach().cpu()\n","\n","else:\n","  #check for real image\n","  idx = []\n","  #train_set = datasets.ImageFolder(\"main_folder/100/\", transform=transform)\n","  idx = get_indices(train_set, LOAD_ID, idx)\n","  test_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch,\n","                                                sampler = SubsetRandomSampler(idx))\n","  data = next(iter(test_loader))\n","  test_noise, test_class_lable = data\n","  test_img.data.resize_(test_noise.size()).copy_(test_noise)\n","  #print(data[0].size())\n","  print('class label for real', test_class_lable)\n","\n","s_output = discriminator(test_img.detach().to(device))\n","print('Discriminator s o/p', s_output)\n","\n","test_img = test_img.detach().cpu()\n","test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n","plt.axis('off')\n","plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Samples of class 0 found are 60\n","class label for real tensor([0])\n","Discriminator s o/p tensor([[[[0.4945]]]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f6058916b00>"]},"metadata":{"tags":[]},"execution_count":27},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dSZMcyZGlNfYl9wQSQBWW6lq7SBGSp2nhL+j53z1n9qG7D5whu0gWtgISmYncM2PLOVQj7NPnYYaopsiU1Yi+kyfcw93c3A3+VPWpauvu7s4CgUB9aP/cAwgEAqsRizMQqBSxOAOBShGLMxCoFLE4A4FK0S3tbLVad9h2+9b28uJ3nY7/v6DbSZfvdv1Q+v3+cns4Gi23R+ORO24wHKbjBgO/D+e4vrnJDnGIc/R6PT/8dhp/v9d3+8bj8XJ7a2trub2zu+uO2xhvLLc7nY6/+B3Oj/FubPj77GJcr1+/dvvOTk+X2/PFfLn95Rdf+XPg2heXV27fYrFIQ8Kznc4m7riLy4vl9snxsdt3inGcn50vt28nt+642Wy28rp67dz4GmOcTN2+ySSN+Uae++1t+pu/mzfOn/6eTWdu32w+55ErN38K7u7uWqv+Pb6cgUCliMUZCFSKIq0lfv3rX7u/Ly4Svfnb939z+9rtRJ9GoKRbW5vuONLJvlBSUlRSzW7X007SxLbQ5jb29XGtTlvoNSg1x2Tm6bz+jve2vZOo7Pbmjjuu1yN997R2KpRpOV6Zj3v795fbN9eeqj04eJD2gbYdHBy4466vr5fbSjUXi3SffH4tf5ij6Io7UEPO2+2tXmv1cYrpLNFOpZZtPAulwgvQztuJp+Wk1Lz2ndBajvHmxo//4iJR9qurZB5wfvVaDaxBgePLGQhUilicgUCliMUZCFSKcigFYYTf//73bt+7o6PlNl3oZmYbmxvYTnbmeDR2x/X7sCUlhNGB3dOBnaY2Sht/6zkGsNt8mMWfYzZPtoGGdHgODYPsbCfbcntrO/2maLf6a/doQ6fIlfUkbDOHHdUIa8GA4S4NP2ziWVxf+1AK7TbeJ+1lM7PRItnZDCWZ+Tk+Rpjl/PzcHcdrMXxk5kMac9hsDbsSx03ErqQ9Ohz4Z8F30+0T05fvnNr4r169Wm6/f/9+uf3u8J07juPSOdD5X4X4cgYClSIWZyBQKYq0lrTlj3/8o9tHpc63v/5V9nekLYO+Dw/0Bmlfu+X/n1jcUbGSthdzT9Xokh4MRSEEmsVx6LWoqlEwXLK3f8/t28R9MsTTlpALXfu8LzOzu1YKFzCE0RbqOp2m4y6vLt2+m5vkwifl3dvz4yU11jG6kBT2Kc33f/tzkObv7e4tt8/Oz9xxDItMJdzA++ZcteSZ0STS53lneVVaD+Pn+zGde5XR7nYKjdEcMDP7l//1L+l8MD+++eYbd9zGRqLQ74485X37wxv7GOLLGQhUilicgUCliMUZCFSKos35z//8P5fbn/3DZ27fHLbf+Zm3KWgr0GYbi/SrZAfS7tHAAcHwCW02M7PLqyQxdDaLSKfaCG9wvGZezjeUUBBPNL1NNgszN8z8+G+n3u1/A8nX5mbKbGFo5sfxp7No6IBhi+GI4QE/c7QrBxJimGJc1wgdTGW8lETubO+5fRsbCJvhWautThtcJW6LRZpTSh07Hc0W4ti9vXiJ+b9b+IdNeSNtwqH4K54/f77c/vqbr92+vb1033xfPv30sTuOD76RddX1IaRViC9nIFApYnEGApWiSGv39tPne2t7y+3b3UquZs1wuMHfc7jNO+qW7+SVP7lE7JYobLbh8taMj8mEYYr8/0ONBGiAYYpLoatXlymk8fzli+X24eFh9nybG57aM0mbydy8rpnZwYOHy+0njz19YlbKDJkc+7uedpL+TSaeTr5582bl9uW1D9vwuTx69Mjte/Y0mT6O4oo5QMqrCiRmsPgsD09PfTaPfyeo2qFpY+bNG2YZ/e53v3PH/e3775fbfVGecf5nM4SnxDS7wLU1iX9TMrRWIb6cgUCliMUZCFSKIq2lgHs+8yoaJsJ2hZps4RM+AcW9vPQU6eIcdFiEx8NB8oLx/EqRlMoSTp0ENYgKtnmO4+OT7PnU40bPJT2o6oGkaFu9tUMcywT2XalDxPkfike53U7jms3o4fTjpRf9+NhTbyYl0+OrZkQpydnX58F9ismyv7+/3H4Aum7m6R9p7ZEobJgAPZJ3gkq0W6khRMXQcJjmUef7n/7HPy23NQqwj8R3qq4aiQY7STF1ISqpH37wdaBWIb6cgUCliMUZCFSKWJyBQKUo2pwduIbVThu5v1Xdk2wMqvbJ8c3MJnDtX0gyKrMVaC9qSOTtmx+W21p7dAuKm93dZyvPZ2b29u1bbPtsgZvbZPdoEagWbBGqRjSBmCEMHT8VJrTvbqQoFm33K7Hdc4WkFgv/77QD1X7eROiDtpMWIFsgg0dtTiqjFvO0byKhtsN3ab5PEPYw8+EYF0Ir1E2ez/0YtxH202fBjB5mVl1c+DmlE+TlK28f0sYt2eC0R6e33vblu5lDfDkDgUoRizMQqBRFWkt6oELpzY20T13IFEv3Qdu05izd5koJSBdICw+FdvJ3KlrfhHicFOPP3/3ZHffXv3y33NYaq6XEY4YqWGdmcuvDJaxHUzoHKdi8EY5J21oflSEYJga3Wj4EwGuNxMTgORgSoeLox3Gkgcwk8Z01dM5R21VrTDFh/m/b37t9DH188eWXy+19Ec+bpRpW2o6BYTmaG2Zmjz75ZOU+KprMVKmk4ZjVdZnVvDh8l8JVmtCvdHsV4ssZCFSKWJyBQKWIxRkIVIqizbmD/h8qm2OiqtpR7B9Bm2U08hkZm5v5vhtHqItLaVxH5Hr9duLuu3v7bl8PRaCev0jJs9+JzcksEk1k9uP1domzcWHnzObaMq7QMwPZFrTrNXRA+/zevftu3xZkYluwnUYj7yfgs1BZ3sn7JFtkqEZbJzrJ4jsvAWS7vVvI665ufI1W1iTu3Xg/xPMXyQZlyIu2opnZ7m561u9Pjty+uUvm9vbo0NgzJ41jQ7KFmGSvfghf1zeNX3uqsDatFqYr9Yj5gPhyBgKVIhZnIFApirSW9EBdv74Ls6cEzPJggIRJwWbe9fxKVBhn6IzMdgNah4h/a7jnhx9S2fxXSIZWmuKyRhrKnERzNatmBOozGmt9odXn13o3kynPn8IZY3Ht++wbf59UrJTMDVKpM1FkMdzBbQ2TbW2n8JSGDtxcQYnTUDAVogic/1evXy63tTUjqb1S13NkgKgii+/0q1fpndCslwO8q6qOY33kBbKFbm99iKuP7JjFXJVWUn95BeLLGQhUilicgUClKNLaritd72kFJSsNUTy8mI4CiMeKtV7OJRnVXQrqisHA09pt0KwzKdH5GgmtbAlAb7KZ9zLqPipnFEzQJX1XCkaqxppBZl6p44TSIhb34nnvsV6HIplp4ru/L5opHGMzcZxtMryq6wrKpZOT5P1VdQy7Y/elm1quntPLwQv39737STG0KSJy3qcMMdvd61aE6azh9MUXX7l9/UV6p/3pvQeWXcxu5bqlmlbLYz56RCAQ+FkQizMQqBSxOAOBSlG0OYewDdQW0C7SBG2R0/fJLc+kZrNycSQXjrlL/4fsQA1j5u27P//5T27f8XFSjrAWq9qRtAmbGTbJ1uN4zbx9WsywQdJ6Q/mDTB1mjWjrOra/UDuQWSpMaO/3/b0w1KFKqFwHbw0tMcxCFZeZtzP5O63ZymvrOfhs6AvQc9C/8Mkjrx6izamtQvhsXMdxLYaGNg5sd2Hm56qkKKNd2ZeMKW19uPL3Hz0iEAj8LIjFGQhUiiKtHbnaPZ7Wsoy+fqJZ5p5UNlfrxqypZiH9II3Y3fW0llTq9etXbh+Tf09Bb24kWblUB4YhBqW8pG5np+n82rqClOy91MzhPtZw1cR01mUaDlU9lI7d3ER9GzFFLqBw0hYapO+8r3fvvHKGSQJ6L0x6Zu1brdk0g3mgtJDvCOdG6e9rqL8ePfRtIWj66PldmAX/ztq/ZmYthEVuRPyfp8b5EFdPnqe+76sQX85AoFLE4gwEKkUszkCgUhSJL2vOak8StvMbSMYKCyLRhijVHm30IYEduLObJHoLsQn/guJcLCplZtZGOGIMV/adyAhvndxL9F6A2k68n9Lv+oM0P7eSkEu7jed7/PhJ9lrq9h+P+Xc6TkNGp2cpDEIb2cyHSJh980bCX9eXKXykCduDAWsUJ5uz8WxZF9c8aOPz/JrNQ9v3UOziT9CaUKWUs9lqqaOGrvj+qa+B7zSPUxkrbVUdPzNWcogvZyBQKWJxBgKVotyOocMaK/6TTbqgLmQq/LlPlfglKsjMFrYDPHrn1RrvTxMt7Am95jl8iwFPMUhrdR9/p255uvpLNWd4Tq2tS+rGsJDSsSeguTpvDGvx2hfnntaeQDGlqhf+jtsbxSRyT/eYdeRURjJvbVD0scwH3zMmkWsy+wxtIt5IO72HD1NIii0XzLzJRVNBTa5SeI10tfR+M/yoa0TbFq5CfDkDgUoRizMQqBRr01pNaKWYW7s8kQqq0mVd0DPqat+ceVUKBcpdoQ4sfUhKei71cyhg12Trq4t8LRzOz/ZO8ig/eOi7NfNe1PPHGjQd0GT1cPJ3Ot+OnoEma80jzoGOg5SM52sqt9LvbqVNAT3Rh2+Sl1evRWq/LYov7qPZMBKaz/HqfbJDW7/nvaJ8ZhyX0toS5WXnNSYdNOpDwVzSdXBw4MubrkJ8OQOBShGLMxCoFLE4A4FKUbQ5yclVbcIsib4U/6LaQpVF64JKEbqyr6RQEgth6RjZaZnu72bRqtVZBmZmrUIhJtobrK2rbn8maTdURjg/Qwy9QuErde2zbitbRowljMA2eq9f+/CD2tofoO0GT45SuGc29SEStiPIqWh0n9qjVD+5AnNDP2+cx0ZHadiEQ1Hi8JyljuOlUEouBNgWxRTfubs7P7/93sfXRXw5A4FKEYszEKgURVrrukaJ6oVl/3WFl2rm5KAKCipkziDYvr7yNOsIShcVvpO2kM6o+oY0RYX1LtSh4miOGb9TKkhaq3WI9lD7divTidvMh4yU8g6H6W+GdDTEcA+0VjuVvXr1b8tt1n26M6V0iYZyTGY+0WDYQ6JBg7rmlTOcYz4LFerP8Vx2tn04hqEmrT3E90DnmCiaOpl3utVqy99pWxMeXr7yhQFWIb6cgUCliMUZCFSKWJyBQKVYuwVgo64suLymr7rwwBo9IcyaLQZpG1BudyY9VZgJcac2EK5dSv6lW16LORmKRaktmbM9NPmctlOpTwsLX33+xRf+HAvWrfUysV7fF/zKgfOhtth7V3M2hUjGko3E/jCUsf04RthwnBux4ynRG0v7SA01fYCGvzhvzEz6cR9kc1pvGc+eckZ9Tznfuq/TSfc2n+dlfqVv3zqemPhyBgKVIhZnIFApirSWn+lGFgO+y/qJnrdLn3qeP22z/oyZpzGsb9MRiuFaDEry7zXauDGDQu+FYyzRmxJIE7VVBakUWxaa+RpCnlJLgi+UUY2EbYx/jnnTLAmqqba2PBVmJg3pu9JfhhiYHK7jIHp9fw5SVzUxukgcp2JKzR7OgV6XXaqfPXvm9tG88QqefOK/JpUTpTpSi7tE81Vp1ipc7wPiyxkIVIpYnIFApYjFGQhUio83bMjAZYrrPlttwyknZ8aK2hSuLR/CJWpv0X5puNtxjkuEMLQ9PW0zDXVwzA2bAuOirbS16e25Tz79dLmt98lzsu4p29GbWdn3jmHN53n7aIBzamGtvb295TZDV2pzTzCnpydalYLzkexMDf2wHfv2nlRCmGKMnA8ZL23H8diHYyjjVNkpw1yUDmrxNm/HNt/wD2ARr8XCn+POhWOkZvMaroz4cgYClSIWZyBQKYq0tqTucap9+WSby0opXBxu9FL7NGZGKLVkcq62pHuHkv1sf6chhlZBzcLQhLrDSVVIYY4lxNAH9X7y1LdZIM29fz9limid4C4yUfS55IpRDYUak/6x67eZDyu4QmBCk0n7Nasj10ZQwXZ474/8XJ11ksmxiXCPPncqlTS85msUe6o5znRrL4X8GuG1jKnTzODJZ7Zo2G/ldT96RCAQ+FkQizMQqBQfobX5Tz0/7a3FepRAP+VtV2vIq0hmSJhlewd2yjIze/8+0SJNLp5m1DLqkSUNKnkn1WtHdRK9h6r9oJKGXlGzfL1YvRa9q9qhisnX7gzyXy+F6ROhe0fo1EWFELtEm/nWFerhJI0jXe/25TXDIDWZQIXwueNowmh3r3vwFCu93tkBHS7Q2pJJt5iv7tCuSeXuYdwprQ2FUCDwi0UszkCgUsTiDAQqxUfq1iZerMoThhjaXc1cQA8K2K0d0+7Y+d4gbPH27ii5/dnR2MwX/NJzMBuCdomqdFzYplETNo1DbSz2YmFSsto5HNeVFv/C+dW+Izis2cyPY04FDkybmYSMaGtrUjnvewCbtitdtKlcOnjwwO2j3c0Qhoagri7zGTbDUToHn5Nmx7j3Q94Jhmo0IbwDVRpVTBrKK3W25rPgvJULhmnoJGzOQOAXi1icgUClKNJafoobom+2pGt89hFmyYjgzcz6oB+67wp09fgk0dpGjR/QOO1iTCp0fpbE0NO5p3ukSCUB9KYI2l2LOqFnBClTg76D1vokZE/jfMhFFSt4FrO0fSO1UhlKGUtn5W++/TZdC/es80HKrq0U7zK1e7VNBqlrSdBOWqu1hnnPaipQ+H5+oTWn0hwMWTtKzlFKvs4dp4kXvJaerq1xrhWIL2cgUClicQYClSIWZyBQKT5ic6ZtbXU+cz1ESq3lVyemmnmbQt3Ql5fJbuh103Fa15SyvIaIsIP+Igir3Jx7u5XnmNx6G4uSQC1Utb+/v/Icaj8fHBwst1W+x/DG/fvpOJWk9Zzb3z822qC8tErSNja20nXFxtreSvs0rEDQltQCXyfoW3NxkeZNe8zwGXbkneA+2vilFn26j3b96XutaZuevZMYii9Abe11oGEb1lFuy5w2pH4rEF/OQKBSxOIMBCrF2jWElHbOnZt4vbqeSmtJR6ZSZ4bnJ8W7uvKZJ3TtK1W7ytQN0qwUnkPpxiW6Y09PPNVhW7od1MLZkJo2W6CMSmtJ3dgOcCShjlKWBGkjKamGH3gOrVHE61G1pNkrL1+8XG5/95//6fYxXEU0lFug6Hv3fIyBSiu+A0q1+V5tSCbL3m6aY1X3MKvpwUGq1auqsRKt9aEsnP8uH1LU92qdUE18OQOBShGLMxCoFGt3GVP1Q0nkm6tp0xavnW8jkKfN/R6FzJ7+vkOSsNYQIjj+RqIr/lYFEunUXGj5FTy5VNUMJRn6+CR5MVlC08zs0cNHK6+lNI60q+m5TOdkudGO0Mk5aL8+z2u0eyD1YxlLM7MbJL6reUBQ+TMc5ZVbep9UE/FZqOeZNYTu3bvn9g1A53vqhYU3nvRUhfUl4XuufYcSVffO6TnW6L4XX85AoFLE4gwEKkUszkCgUhRtzqvrZFNpuKRkcxLk51pf1HUZFluSmS5s5ad2JRUr6v7mPmc7algI1+qJS32Kc6oNR5uC11J7Ynsrtf2biYqEhbWePE41bZvzzTHqY1tdH1W7OtNe127QVEL98PqH5TZbSZiZPX36dLn97vCd28dz0PZVdAp2cW6884V/Zt2L1Z3PzcweY8jqX8jVktVwD9+XddtAauEvdiq/vs77MnKIL2cgUClicQYClaJIay/h1tbkUIp81S3PT3a+LqunNFrv5vg4USZ2tlaKUao5O52kcy66aZ9SCnabVsE5qZWGWUiHRxBsP3v2mTtu/14SyJdCUi60JDWbWJOnJ+EY3vesoKqhkki7jA0QMiFFv5ZwyQjzc//gvtvnwl9Dbx648eLe9LmvrgjbTHggDV2ImXIIU0GT1nmsCzs15grnL9BaqqnUXPLPM9+6Iof4cgYClSIWZyBQKWJxBgKVomhzskAUEivMzHcrnokLmXYmubsm3VLyptkmx0jcpeRKXd78W7MTaKfkJFcK3cfMDrVLaEc8epRkeF9+9WV2jG/fvnX7KHNjvdVSHVUFTfkO6wRLOIM9VdSGG2+wF0s6TmvwclzPnj3L7uPvSmEDDX9pv5scvCxUZJWwk98e+vl+8iSFgkr+EEpNOwttTwlZa6Zvipl/vzULZTLJt0hc/v6jRwQCgZ8FsTgDgUrxkawUdHWWzz5DAnMJD1hvdU0hrX1DWshwiZl357t6qxLqYHaCtgdsT1ZnFqhr3NWjEeqtih6CmQzcVrc5z6H0ZnMTtXtAf0vdq7VYUtuZDmlbMy0YLlHzgAnirI10ITSzheeiNWeZHXICJZfWhC2py5iMzjHqePkeNCgjwhTaOpCU16nXNJTCbBN5bxnWYrhOkcvOMjObFujwB8SXMxCoFLE4A4FKUaS1pK76WSY11JKAg8Hq+ihKHejRu7wUJQpq3NCjdycUmh5a9SwygZvqnhsRIZM2K0UqUTB2YX78JInWtf1ArhyjmU8GLidUY+5keruZdg+aKD1Gu4qNDd9aYgv0mvT0T//7/7jjRqjxox5felqpLNKkbL5LSo35N++ldJy+V6WucRwLkwtKydattjeDuC5uWH9qTZG9mXZMX434cgYClSIWZyBQKWJxBgKV4iPtGFwLX7ePtpiqPMj52dlaeTdtM+XrLAp1du7buOXGqCES1qo9x3ZjvMySKGTY9AbeXnz0ySfLbdacPZGE8DOEidR2enDA7tD5Gr+jUbLvBmJLuvYGLiPIzzdtuF4330JjF3Vft6XbNpUtOt+05zjHh2+8SmcwTOPVUAezNRhWURu81FbRhbikhQbDg7e36Vp9Oc4Xn5PWfvS3TNazORXrtHuIL2cgUClicQYClaJIa6mW0VosJXc1KW+7oJIgpdGQBRNhSTUpiDfz4RNNhmZogiEAk3GQtmhNGx6q7nae33Vylpo2vG+lgrw3bmsNnpKaJa9E8ffJ56RicXa6nmwmyvX02VN33L/+4V/TGF+/dvv4DEnfP3ns6xBpzZ8ceJzSWt5LKeFBO3h7UT9DXHlaq2AIhgkgjUIANLkKLUtyiC9nIFApYnEGApUiFmcgUCnKLQALna1LGSvcx0JVyslpI+r5b7GPtWoZHvnxd6sLZJl5Fzttj35PJHSwJdXFzTGqXI21VNcNLW2hmJiZbw/IdoB6LZfALnaxT25H6CeTHbRqH8d1hb4pzFAxM/vss5RgfXh46Pa1cO0p7MWh3EsfISl2fzbzye3cnkorQvbI0XAMn/s9GT/lkrQDF4t8ca6WFLdjYTdua5s/Fltbt84zEV/OQKBSxOIMBCpFkdZ6JYSnFaxHU1JG3DUaoyWwRpGqgE7RNbqUKE3aqfs4DrrQ1fXu3PKbPltjUqiFwzYLU4RBdkRVwxDJwcGB28fsEF5bu1KTZmlbuxyV1dqopeTlLbaMYOtHoWNsr/f61Su37/nz58ttJlhre8eBtEgkrjNtCkvJ53qfJycny+37931tXbbKaFk+7OTfOU2eR/0s3JsGX5iIPSt0bs8hvpyBQKWIxRkIVIpyDaFCErLvTlygWVC66Dkur5Lw/d077/mbwDvH35XK2uv5qVIZIdFYvW/dQkepUsKv60SFcVGwbea9kwf3Pa0lNe4XvKv00Gr3MIrkS0oieqn1Xkgh9yB8b7RLwN/tx57IvXnzZrnN56IeX3qzNUGeYLKCGkc0TfQ+aRKod/96P5lBfF+U5hOaDLGYI4ka/651gUhlacKtOucqxJczEKgUsTgDgUoRizMQqBRlm3OWVzjQbmsI+NkewHWU9jybdmajYzVCJB3YVBoGYf3SuWYFsB0ezqFhCtqZmjFRyk6g3cZzDuT8u9sptEJ7zsxsYzPdzwC2qV6X4ZOuFqNC9hBDDppo3B+kvzULg7VeOR+jkb+XXAEuM7Ovv/5muf0f//Hv6dwShuM59D5z811KTi7Zb2rTHqG1JG1CLbzG91Zt2m4PNj7me34r/pAZE7t9xlTJ1v6A+HIGApUiFmcgUCnKdWsX6bM8bSgaoAJSPzf+Zgjg6tILlE9BZbXWK+nNzm6iheMNnzxLOqwdqqgeIu3Ua1FRojdDCqldu9hhiuNQik511XDoQxj7+6lG7AIicBW3k3b1RWHTzXQS70gdokJJKGdykNZ2OvkEc6WgDx+mekhHR6m+0itREjHMogJ8n3C+uqavmVkPf2uXbr4jqh5i2w+GWT77LN/VrUS9uUfNKs6phqSC1gYCv2DE4gwEKkUszkCgUqwfShF3tcs8keJFdC/THjo987YY65yq7UH3tcvIkOP4t0rvaG+Q45++960CeS+b276FN0M32u7t+CgVGyu1pGORM+1ofHmZ7N+dnSTlG4qNVS44tTrzR3/jjpPTcR6ZAK3n2NtLUjyO3cxshvlnvxUtyvYWdWz1/NuYA5dwLvc4wPywjaKZ2XCYz3rhO3F8fJQ9riTn8+8+tjWU5zJbtLN11K0NBH6xiMUZCFSKtZOtte5m7tNuJtka+Jy/fevL8jM0oaodhkGotNDWaQyLKFUgNfE1UD2VGiC8oRSdmRaqqiGYYK1u/6G4+gnSbaqwdL5Lyb+tVg/b+awUdz4JjbkEbjxOVd+MRnn6TsrI7Bultazrq2EthqGYRbO3t+uO8+aMvxfOo9JTUuUzjEOzRjh3pbYKNA/0qIXLpvLzqHWPViG+nIFApYjFGQhUirK3ljRrni8n3/ROpjXPkoanp95b68Xz+VYNV+h6PZl56kqP8raUnaRi6Ba0pS/dwtxvLrzK6BpdsAfiKR70ExUnrd2UOkSkZyyFaWY2gpeXSettSQgvK1ZWH9c0N/KieLtLJ+l2oRZSc4Z/iseX9YZoijx88NAdR1NB20LwudPsUcpIdY92zqaJxGR2M+9Vn+GdVnVZLoH9v/4ljWtxt3LbzGzOaIGYXHq9VYgvZyBQKWJxBgKVIhZnIFAp1rY51X7RloAEQxUMidxKiz66lzUMQjviAkoU5f9M3FWFEN30tF8aSiJcW1UeYyQbb4gSZW8/JU7v7iRXP4uJ6fXGY38O1nDlvRWc943Ed1WfrDqfmd6WE7EAAA1pSURBVLejBgM/RtpL02n63ahRPxe2r4SkWAeWCh4Nf714+WK5/fLFC7eP9nqpBi/fKw0Z0Z7TuWLbiQX8FZoMPUY94ZLS6s79u9S3xfk1dHJ2mu/W/gHx5QwEKkUszkCgUpQVQo7W5uvzlBQUVMDc3ubdyUyC1X2kNNp9i1BqTJc6qZVSDLrXR1KjaJTpemXmwyJMvG60H4AQe0OSxce43sApi/JzWkr+1bYFBLtl94QKTpBY79Vf/hx8DzS85s2g9O/anuLp09Qtm93CzLyZQqWVS4g3sxvQUK1DzORrDVnwPKwvzJCZmW9PYXJ+NwfsMmYeVFepEmqd7t7x5QwEKkUszkCgUsTiDAQqxUd6peSTRR3XVvkeXPY3N4nzX155/n98krIVSi3RaEfdSDjGtSnUvh7g/GyvtykSOtqxKgVz/TREvkd7mnIyzUoZ3A3wGz9+joVSOZWC+e7V/rHRBOW22ot8hpOpz5KYTBDycr1MvB3fdq33/D7ON3vdzORau6jd++DBA7ePGSx8LlqveF7oGu1kpxIau0bX7rdvk4xQpaWffJIKlKl4z7fzy2dn8X28lvdKw0urEF/OQKBSxOIMBCpFkdZOZ/n2evy0l1o1sM6MtvljtkmjzRroCEMkqgbhPg2z5DI0NBTRL9CnTqE9IOFqmYrrnTVW21JL1qmT7lKYRSn0ujWEXJmgwm+007JL5mZIROh1q5vPeuE5WKd1OtPWeKvNDTPfHZsKIb0T0mE1dWhi6LvJMNrRUaoh9OrVS3fct9/+ynLg+KmU03f4+iZd61JpbdQQCgR+uYjFGQhUinI7hulq1YiZV8FoDRcqKEgd3p94jxgF7aqqmTtRcvJsTYUO0NupidJs4/D408fLbRWm07uqpTfZ3fv6yiuLqPLgGK+FZt1Dy4WutHTIexb9fHP+db4Xi9W1gpTW8ndKa/358+okirn1nfCe4nQvWj+H9K/dEOen78URSo/u7nqV0RaemZozfJ7a2ZpKHXpTn4sAX2sKEc5TjPlQD6xL3jj3CqGpzP8qxJczEKgUsTgDgUoRizMQqBRFm5PuXnVJzwqKHto2bw+TCoOuZTNvIzYTiOmWh+0r12JGyWfPPnP7HjxMhaUYIulI5kaHrQjmEjKaJ9tgY+ztXWY1sCCXdrZmS4qZ2NY0cZmsrCqgXHsKRSlDxc2p2IF3mSjRTwmhuaJssLfOzr3dRzWOtsKjvchE7PcnvvbtJWzHbcl6yWW2mJmd4Dy0OY+OfHbM+XnKkmr6CTCP7F4trTbOz5JP4lKyUkp2/QfElzMQqBSxOAOBSlGktaz1qvViJ6AESrOuEHKgkFlp0BVEyCqOJnqDxP0+/4fP3b4vv/xqub3V6DaV6GWvx5qwfrxU4zRbOiQ6rJ3W3HGgT9pRmvWLVOHEv9trUldVGeWgpyglUfvzp/+zNQzC+WnWfULy/FmihZrwXEqef/LkyXKbydx//ctf3HEvXyRFjybPsxbwSFph9LrpOZF66xhJc/f27rl9fI9pHihFpzqu0cm69AD+C/HlDAQqRSzOQKBSxOIMBCpF0eZk6GMiSbczl9jsjRu2ccsV6jJTCaD35dNW+O1vf7vcVpuz7zI+/P81LM7VQSs4LVbWRw3X8dhnpfRRV1btL8K3jPP7KAmkPWRmNoTNVeqO7a/l7zNnn7ZkPvquRq4/luEBPotuN58do2EKttvro4/MwX2fUO0SscWOn6PQ2Oefp2f91VdfueO+++675fbhoW8tyY7jGmbZ2EzPl74GtTnfHqYMKiaHm3k7kwnnavteIFS4jlxPEV/OQKBSxOIMBCpFmdbiM621ZOaFxOP371NtUFLjUv0fzQb5zW9+s9z+9h+/XW5rewDyM+1i3AON67K9ntA9hlxK3aCbWRhQ9ODa3UK4RNUmQ3TV1jnIQVVGXSRAO/NAmLHLvhG6upin58s2CwxBrfqb4DxuSes9gnWUml2p0/j57jwQavwQ6q/nL567fd9///1y+6/f+RAMFU67UHjps2UIcCLmGBOsbwv1kBlaKiXq5xBfzkCgUsTiDAQqRSzOQKBSFG1OZo3f3Ho+vZjnqyScQbp1fXmdPY6Ftb4WV/kXX3y53GaIQW2lIUIupUwO2qNqVzJsoxJDJ68TW7XrwjMoINZW3ZytPO7Hc64OTQwG0noPx+nped8Ms3R6KiPkeP05XHWCdsaGNR9yYA1YM1/YbG9/f7m9LXWC2+0U3lBfBiWB40l67psbvhBYt1C9gj1stL/Nn//0p+U2s6I4XjOzc2TSaPUNX7s32aNXDZszzU8pNJZDfDkDgUoRizMQqBRFWsvP/o20SJsV2gOyRiczT1RRcv/+/eX2N1//o9vHTtHDUVJ1DCWLgZRGQymkez1QvI6GXNzfedWL0lpfFCtfnIvue6Xe8/lqF/toNJTj0nyruofndCodCXuQQqsyZzDgfafjWKDNzOz165QNMpn4EMPmVqKepJNjuZdtdJdmKMnMbIrspJsbhvIkqwNzsNDEcW4LnWSiPUMuWpyL19NQCi9AGn4tKiOnqvvprDa+nIFArYjFGQhUijKtdZ4574miCFwVK7mEXNIZM7Nf/SqVvN/f9+JiCsQ3sN0UW5PWqjIn/d9DSqqtDvq9dE6ljK1WXjFEuurUQh2lk3mPL/eVPMNu/ELLOWZSdm0L0cd9qxeT9O8UtV5Pjj2tpXdS1UItcDetF0uwfs6mJsiDXvf7abyqLuM9axI8Bf7DoTeDOMd8lw6lw7aj/dp2AvdZ8taq6fBTEV/OQKBSxOIMBCpFLM5AoFKsXbeW7frMzKYzdhaWhNn5arXJs6fP3HGPHj5abm9KFgMVIQyXaEEoV7irk7fneI6hqG+8fSc+b/y50KwUdw5siwKJNlZb5D2ljtXuWoV6tJnhNkJcnANV/pyews48SXbmudRb5V1r/V+XPD9P78vJXDuOp7+1SNgOkqOp9Cll+miyec/5JTSGkY4do5bx5R/+4I7iuNR25ymZeH0jNud/JxOFiC9nIFApYnEGApWi3AIQruArETlPUXpeXcbshszwydNnntbuoDaLUk26uRn6UJFzr08Fj6c+rFFKBYyGIshTGmXyQWW72t6gu5paNdogQAWkFIwUmKqgUqijIWgH7eqjxm8jcZzhB60hjMTgy0ybPDM/j6q+0WM/oNG13M2HzJVrzZ02t8Xs8QqqvKmg9ZA55k3Uizr78tQdx/aUaoq4Voo4v9bZUjPopyK+nIFApYjFGQhUilicgUClKNqci0LLddoX2g6wi/AG2703epnAzlRZnrM5uwyJaEZJf+VvzLzd1rQzE2h/tVsSLmmvlgAqSr1NFp28S73TXR3uUbg27qIK6/RXj1FHxHCESikZOmCWDtsomvnwhs4HbUvOt84953s89knUizs+i3QHd42wBJPU/bwxeV57pfC97eLdYY1cM5+x0phJ2JIMH/29cj1FfDkDgUoRizMQqBRFWsuQiCa7TpFoq+5qZhqQVigNKil/HC3q5akfqauGH/I1aD117bpMDk9hqNpRBRLh2wpqsnWiZHoOUk2velG1U9pW5Qld/eyOrSk2rLvDRHczs3v3Upu7UgZMSZmzriKGtFZDOuwOzWwTDdP4d06eJ7KMNjc9bdbslg/QOrv3Dw6W26TaCt+aQUNJEUoJBP6/RCzOQKBSlGktk0pFoHw7zSuE7t9LlOn0NHUcUwUPKZKvYePpK3+nJSNHrjSmP3/Og6rjaLmWDv7/qxLVdCUwCwyGTuRSEvVwlKi9dt/mo1L66OocdeitFVqbSQQw82YFk9QbbTco4JEx8m+OsUR3G88If86Q6K5iG9Yamkqyfwfn1K5x43GKOvC9VZNoFwL8+cyPnyoj1+VavbV/H6uNL2cgUCticQYClSIWZyBQKYo2Z64+p5lX4M8kmZahlEuo+1XdQ3tLE5RZuIvKH1V80HZqJMXyWrCjSom7are2CwqhkirIn389ZRGLUZVaETYyWzJjVJuz1VodRjDz89jrp2trlo63v/z51205UDqui1ey1YdCSI6bF+om8/T9vn/nNtHZmpk4+l4RzawUqLVoc87zIZf/DuLLGQhUilicgUClKNNaYDrz1JWKoamU5SeNG41SzdmudAhzyhxNmAVFHQzy4nYNi7hxICzC7mRKGUsdyJz6phBKYeij3cnT3dmsIIJnTdVeP3uchlmY1FvqEEaBeElkT9rc08TuTj5xnBR4MUfC8yyvaNJ2FBwzz69UeD7P10NiSEN/x/YPDJ9cX/vjdlEIYKY1spAQwnWhaqe/F/HlDAQqRSzOQKBSxOIMBCrF2jbnXCRSPtlak0yTrcCwSjPDob1y28wX7urB/mpKABE60CwMhG563TVbBfZLNmderlbK1vDudh+ScmCN3IWfbw01uZ/BruKzaHTpRmL3xqaXtbl+K2v2bCkln7tO2R0/jtkc9vlMbcnVCcsaLul0+DzncizmQ3wllGduoG6thgo5B5OpFO6CjcvsmL+3Tq0ivpyBQKWIxRkIVIrWuqqOQCDw/xbx5QwEKkUszkCgUsTiDAQqRSzOQKBSxOIMBCpFLM5AoFL8X/UwcKPPKno2AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"Cpf-AsWkqzZf","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"df8ffdac-678a-42f9-b83f-cf06a500deed"},"source":["test_fake(generator, discriminator, metrics, 500, \"normal\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing Block.........\n","Discriminator_mean:  0.5166683793067932\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r_RDk-5_izjj","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"671b61ee-3715-40ad-e821-c06b8a89ce00"},"source":["#To confimr no of images in a specified class\n","path, dirs, files = next(os.walk(os.path.join(data_dir, \"train/pneumonia_vir\")))\n","print(len(files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["71\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8la1EFeB3zRF","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c864d9ed-c889-4244-a5c4-db51946ca646"},"source":["from torchsummary import summary\n","\n","summary(generator, (100, 1, 1))\n","summary(discriminator, (1, 64, 64))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Sequential: 1-1                        [-1, 1, 64, 64]           --\n","|    └─ConvTranspose2d: 2-1              [-1, 512, 4, 4]           819,200\n","|    └─BatchNorm2d: 2-2                  [-1, 512, 4, 4]           1,024\n","|    └─ReLU: 2-3                         [-1, 512, 4, 4]           --\n","|    └─ConvTranspose2d: 2-4              [-1, 256, 8, 8]           2,097,152\n","|    └─BatchNorm2d: 2-5                  [-1, 256, 8, 8]           512\n","|    └─ReLU: 2-6                         [-1, 256, 8, 8]           --\n","|    └─ConvTranspose2d: 2-7              [-1, 128, 16, 16]         524,288\n","|    └─BatchNorm2d: 2-8                  [-1, 128, 16, 16]         256\n","|    └─ReLU: 2-9                         [-1, 128, 16, 16]         --\n","|    └─ConvTranspose2d: 2-10             [-1, 64, 32, 32]          131,072\n","|    └─BatchNorm2d: 2-11                 [-1, 64, 32, 32]          128\n","|    └─ReLU: 2-12                        [-1, 64, 32, 32]          --\n","|    └─ConvTranspose2d: 2-13             [-1, 1, 64, 64]           1,024\n","|    └─Tanh: 2-14                        [-1, 1, 64, 64]           --\n","==========================================================================================\n","Total params: 3,574,656\n","Trainable params: 3,574,656\n","Non-trainable params: 0\n","Total mult-adds (M): 423.53\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.91\n","Params size (MB): 13.64\n","Estimated Total Size (MB): 15.54\n","==========================================================================================\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Sequential: 1-1                        [-1, 1, 1, 1]             --\n","|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          1,024\n","|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n","|    └─Dropout: 2-3                      [-1, 64, 32, 32]          --\n","|    └─Conv2d: 2-4                       [-1, 128, 16, 16]         131,072\n","|    └─BatchNorm2d: 2-5                  [-1, 128, 16, 16]         256\n","|    └─LeakyReLU: 2-6                    [-1, 128, 16, 16]         --\n","|    └─Conv2d: 2-7                       [-1, 256, 8, 8]           524,288\n","|    └─BatchNorm2d: 2-8                  [-1, 256, 8, 8]           512\n","|    └─LeakyReLU: 2-9                    [-1, 256, 8, 8]           --\n","|    └─Dropout: 2-10                     [-1, 256, 8, 8]           --\n","|    └─Conv2d: 2-11                      [-1, 512, 4, 4]           2,097,152\n","|    └─BatchNorm2d: 2-12                 [-1, 512, 4, 4]           1,024\n","|    └─Dropout: 2-13                     [-1, 512, 4, 4]           --\n","|    └─LeakyReLU: 2-14                   [-1, 512, 4, 4]           --\n","|    └─Conv2d: 2-15                      [-1, 1, 1, 1]             8,192\n","|    └─Sigmoid: 2-16                     [-1, 1, 1, 1]             --\n","==========================================================================================\n","Total params: 2,763,520\n","Trainable params: 2,763,520\n","Non-trainable params: 0\n","Total mult-adds (M): 104.48\n","==========================================================================================\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 1.38\n","Params size (MB): 10.54\n","Estimated Total Size (MB): 11.93\n","==========================================================================================\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Sequential: 1-1                        [-1, 1, 1, 1]             --\n","|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          1,024\n","|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n","|    └─Dropout: 2-3                      [-1, 64, 32, 32]          --\n","|    └─Conv2d: 2-4                       [-1, 128, 16, 16]         131,072\n","|    └─BatchNorm2d: 2-5                  [-1, 128, 16, 16]         256\n","|    └─LeakyReLU: 2-6                    [-1, 128, 16, 16]         --\n","|    └─Conv2d: 2-7                       [-1, 256, 8, 8]           524,288\n","|    └─BatchNorm2d: 2-8                  [-1, 256, 8, 8]           512\n","|    └─LeakyReLU: 2-9                    [-1, 256, 8, 8]           --\n","|    └─Dropout: 2-10                     [-1, 256, 8, 8]           --\n","|    └─Conv2d: 2-11                      [-1, 512, 4, 4]           2,097,152\n","|    └─BatchNorm2d: 2-12                 [-1, 512, 4, 4]           1,024\n","|    └─Dropout: 2-13                     [-1, 512, 4, 4]           --\n","|    └─LeakyReLU: 2-14                   [-1, 512, 4, 4]           --\n","|    └─Conv2d: 2-15                      [-1, 1, 1, 1]             8,192\n","|    └─Sigmoid: 2-16                     [-1, 1, 1, 1]             --\n","==========================================================================================\n","Total params: 2,763,520\n","Trainable params: 2,763,520\n","Non-trainable params: 0\n","Total mult-adds (M): 104.48\n","==========================================================================================\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 1.38\n","Params size (MB): 10.54\n","Estimated Total Size (MB): 11.93\n","=========================================================================================="]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"4N0yiqHfzOzX","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"9599816b-3707-49c0-c869-932427a16458"},"source":["print(generator)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Generator(\n","  (main): Sequential(\n","    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (13): Tanh()\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o-Z61n2SAhlv","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"671b8da4-7a8b-4136-948f-9b057aa52075"},"source":["print(discriminator)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Discriminator(\n","  (main): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (12): Sigmoid()\n","  )\n",")\n"],"name":"stdout"}]}]}
