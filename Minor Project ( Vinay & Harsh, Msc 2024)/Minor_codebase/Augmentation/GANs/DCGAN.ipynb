{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLM6o1DSjcXJ"
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7QF4YbN1Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeffc764-7316-45f2-c235-ef508317ec70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MdEuFt0nkmc"
      },
      "source": [
        "#Setting device to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 200\n",
        "\n",
        "#Flag to load pretrained model\n",
        "LOAD_MODEL = True\n",
        "\n",
        "#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/p_virus_200_2020-08-22_15:49:13.dat' #P_vir_200_opt\n",
        "#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/p_bacteria_200_2020-08-22_16:21:47.dat' #P_bac_200_opt\n",
        "#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/normal_200_2020-08-22_16:38:52.dat' #Normal_200_opt\n",
        "#PATH='/AUGMENTATION_GAN/gan_models/epoch_200/covid_200_2020-08-22_16:58:21.dat' #Covid_200_opt\n",
        "\n",
        "TRAIN_ALL = False\n",
        "#All images will be resized to this size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.002\n",
        "lr_d = 0.0002\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "# Beta2 hyperparam for Adam optimizers\n",
        "beta2 = 0.999\n",
        "\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "# Input to generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device) #batch of 64\n",
        "# Define Loss function\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHXGFAtBHH4R"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def weights_init(m):\n",
        "    \"\"\"\n",
        "    Custom weight initialization for neural network layers.\n",
        "\n",
        "    This function is typically passed to `model.apply()` to initialize all layers\n",
        "    of a PyTorch model according to the type of layer.\n",
        "\n",
        "    - Convolutional layers (`Conv*`) are initialized with a normal distribution\n",
        "      (mean=0.0, std=0.02).\n",
        "    - Batch normalization layers (`BatchNorm*`) are initialized with:\n",
        "        - weights from a normal distribution (mean=1.0, std=0.02)\n",
        "        - biases set to zero.\n",
        "\n",
        "    Args:\n",
        "        m (nn.Module): A module (layer) of the neural network.\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    # Check if the layer is a convolutional layer\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "    # Check if the layer is a batch normalization layer\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4uvXCj5zzNa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "a207d51d-b55b-4b8d-d186-072b1780e1b1"
      },
      "source": [
        "# class Generator(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self._model = nn.Sequential(\n",
        "#             # input is Z, going into a convolution\n",
        "#             #i/p,o/p,kernel size,stride,padding\n",
        "#             nn.ConvTranspose2d( nz, ngf * 16, 4, 1, 0, bias=False),\n",
        "#             nn.BatchNorm2d(ngf * 16),\n",
        "#             nn.ReLU(True),\n",
        "#             # state size. (ngf*16) x 4 x 4\n",
        "#             nn.ConvTranspose2d( ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(ngf * 8),\n",
        "#             nn.ReLU(True),\n",
        "#             # state size. (ngf*8) x 8 x 8\n",
        "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(ngf * 4),\n",
        "#             nn.ReLU(True),\n",
        "#             # state size. (ngf*4) x 16 x 16\n",
        "#             nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(ngf * 2),\n",
        "#             nn.ReLU(True),\n",
        "#             # state size. (ngf*2) x 32 x 32\n",
        "#             nn.ConvTranspose2d( ngf*2, nc, 4, 2, 1, bias=False),\n",
        "#             nn.Tanh()\n",
        "#             # state size. (nc) x 64 x 64\n",
        "#         )\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self._model(input)\n",
        "\n",
        "# Generator function for GAN\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    # Forward Propogation\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c73d6d1e4448>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Generator function for GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEb5BlY5GO4_"
      },
      "source": [
        "# class Discriminator(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(Discriminator, self).__init__()\n",
        "#         self._model = nn.Sequential(\n",
        "#             # input is (nc) x 64 x 64\n",
        "#             nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             # state size. (ndf*2) x 32 x 32\n",
        "#             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(ndf * 4),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             # state size. (ndf*4) x 16 x 16\n",
        "#             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(ndf * 8),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             # state size. (ndf*8) x 8 x 8\n",
        "#             nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(ndf * 16),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             # state size. (ndf*16) x 4 x 4\n",
        "#             nn.Conv2d(ndf * 16, 1, 4, 1, 0, bias=False),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self._model(input)\n",
        "\n",
        "# Discriminator function for GAN\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.Dropout(p=0.25),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # Forward Propogation\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLWLMetCrGe"
      },
      "source": [
        "def plot(name, train_epoch, values, path, save):\n",
        "    \"\"\"\n",
        "    Plots training metrics over epochs and optionally saves the figure.\n",
        "\n",
        "    Parameters:\n",
        "    - name (str): Metric name (e.g., 'loss').\n",
        "    - train_epoch (int): Current epoch number.\n",
        "    - values (list): List of metric values.\n",
        "    - path (str): Directory to save the plot.\n",
        "    - save (bool): If True, saves the plot as a PNG file.\n",
        "    \"\"\"\n",
        "    clear_output(wait=True)\n",
        "    plt.close('all')\n",
        "    fig = plt.figure()\n",
        "    fig = plt.ion()\n",
        "    fig = plt.subplot(1, 1, 1)\n",
        "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
        "    fig = plt.ylabel(name)\n",
        "    fig = plt.xlabel('train_set')\n",
        "    fig = plt.plot(values)\n",
        "    fig = plt.grid()\n",
        "    get_fig = plt.gcf()\n",
        "    fig = plt.draw()  # draw the plot\n",
        "    fig = plt.pause(1)  # show it for 1 second\n",
        "    if save:\n",
        "        now = datetime.datetime.now()\n",
        "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
        "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGbk-t3imrL"
      },
      "source": [
        "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
        "\n",
        "    \"\"\"\n",
        "    Saves model and optimizer states along with training metrics and plots.\n",
        "\n",
        "    Parameters:\n",
        "    - generator (nn.Module): Trained generator model.\n",
        "    - discriminator (nn.Module): Trained discriminator model.\n",
        "    - gen_optimizer (Optimizer): Optimizer for the generator.\n",
        "    - dis_optimizer (Optimizer): Optimizer for the discriminator.\n",
        "    - metrics (dict): Dictionary of training metrics.\n",
        "    - num_epochs (int): Current epoch number.\n",
        "    \"\"\"\n",
        "    # Get current timestamp\n",
        "    now = datetime.datetime.now()\n",
        "\n",
        "    # Get latest generator and discriminator losses for naming\n",
        "    g_losses = metrics['train.G_losses'][-1]\n",
        "    d_losses = metrics['train.D_losses'][-1]\n",
        "\n",
        "    # Format filename using losses, epoch, and timestamp\n",
        "    name = \"%+.3f_%+.3f_%d_%s.dat\" % (g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "    fname = os.path.join('.', 'augGAN/model', name)\n",
        "\n",
        "    # Prepare the model state dictionary for saving\n",
        "    states = {\n",
        "            'state_dict_generator': generator.state_dict(),\n",
        "            'state_dict_discriminator': discriminator.state_dict(),\n",
        "            'gen_optimizer': gen_optimizer.state_dict(),\n",
        "            'dis_optimizer': dis_optimizer.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'train_epoch': num_epochs,\n",
        "            'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
        "    }\n",
        "    # Save model checkpoint\n",
        "    torch.save(states, fname)\n",
        "\n",
        "    # Create a new directory for saving plots\n",
        "    path='augGAN/plots/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "    try:\n",
        "      os.mkdir(os.path.join('.', path))\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "\n",
        "    # Generate and Save plots for training metrics\n",
        "    plot('G_losses', num_epochs, metrics['train.G_losses'], path, True)\n",
        "    plot('D_losses', num_epochs, metrics['train.D_losses'], path, True)\n",
        "    plot('D_x', num_epochs, metrics['train.D_x'], path, True)\n",
        "    plot('D_G_z1', num_epochs, metrics['train.D_G_z1'], path, True)\n",
        "    plot('D_G_z2', num_epochs, metrics['train.D_G_z2'], path, True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn5d4tccMIyL"
      },
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "def train_gan(generator, discriminator, gen_optimizer, dis_optimizer, train_loader, num_epochs, metrics):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "    Trains a GAN using the given generator and discriminator models, optimizers, and dataloader.\n",
        "    Logs performance metrics and saves model checkpoints and sample outputs.\n",
        "    \"\"\"\n",
        "        iters = 0\n",
        "        print(\"GAN training started :D...\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(\"Epoch %d\" %(epoch+1))\n",
        "            # For each batch in the dataloader\n",
        "            for i, data in enumerate(tqdm(train_loader, 0)):\n",
        "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "                ## Train with all-real batch\n",
        "                discriminator.zero_grad()\n",
        "                # Format batch\n",
        "                b_real = data[0].to(device)\n",
        "                b_size = b_real.size(0)\n",
        "                label = torch.full((b_size,), real_label, device=device)\n",
        "                # Forward pass real batch through D\n",
        "                output = discriminator(b_real).view(-1)\n",
        "                # Calculate loss on all-real batch\n",
        "                errD_real = criterion(output, label)\n",
        "                # Calculate gradients for D in backward pass\n",
        "                errD_real.backward()\n",
        "                D_x = output.mean().item()\n",
        "                metrics['train.D_x'].append(D_x)\n",
        "\n",
        "                ## Train with all-fake batch\n",
        "                # Generate batch of latent vectors\n",
        "                noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "                # Generate fake image batch with G\n",
        "                fake = generator(noise)\n",
        "                label.fill_(fake_label)\n",
        "                # Classify all fake batch with D\n",
        "                output = discriminator(fake.detach()).view(-1)\n",
        "                # Calculate D's loss on the all-fake batch\n",
        "                errD_fake = criterion(output, label)\n",
        "                # Calculate the gradients for this batch\n",
        "                errD_fake.backward()\n",
        "                D_G_z1 = output.mean().item()\n",
        "                metrics['train.D_G_z1'].append(D_G_z1)\n",
        "                # Add the gradients from the all-real and all-fake batches\n",
        "                errD = errD_real + errD_fake\n",
        "                # Update D\n",
        "                dis_optimizer.step()\n",
        "                # if i>0:\n",
        "                #     if errD.item()>G_losses[i-1]:\n",
        "                #         dis_optimizer.step()\n",
        "                # else:\n",
        "                #     dis_optimizer.step()\n",
        "\n",
        "                # (2) Update G network: maximize log(D(G(z)))\n",
        "                generator.zero_grad()\n",
        "                label.fill_(real_label)  # fake labels are real for generator cost\n",
        "                # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "                output = discriminator(fake).view(-1)\n",
        "                # Calculate G's loss based on this output\n",
        "                errG = criterion(output, label)\n",
        "                # Calculate gradients for G\n",
        "                errG.backward()\n",
        "                D_G_z2 = output.mean().item()\n",
        "                metrics['train.D_G_z2'].append(D_G_z2)\n",
        "                # Update G\n",
        "                gen_optimizer.step()\n",
        "\n",
        "                # Save Losses for plotting later\n",
        "                G_losses.append(errG.item())\n",
        "                D_losses.append(errD.item())\n",
        "                metrics['train.G_losses'].append(errG.item())\n",
        "                metrics['train.D_losses'].append(errD.item())\n",
        "\n",
        "                # Check how the generator is doing by saving G's output on fixed_noise\n",
        "                if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
        "                    with torch.no_grad():\n",
        "                        fake = generator(fixed_noise).detach().cpu()\n",
        "                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "                iters += 1\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch+1, num_epochs, i, len(train_loader),\n",
        "                    errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPHMkQQKJItb"
      },
      "source": [
        "def get_indices(dataset, class_name, indices):\n",
        "    \"\"\"\n",
        "    Appends indices of samples with the given class label to the provided list.\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset object with a 'targets' attribute.\n",
        "        class_name: Target class label to match.\n",
        "        indices: List to append matching indices to.\n",
        "\n",
        "    Returns:\n",
        "        List of indices corresponding to the specified class.\n",
        "    \"\"\"\n",
        "    j = 0\n",
        "    for i in range(len(dataset.targets)):\n",
        "        if dataset.targets[i] == class_name:\n",
        "            indices.append(i)\n",
        "            j += 1\n",
        "    print(\"Total Samples of class\", class_name,\"found are\",j)\n",
        "    return indices"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekCtxRmkLl5R"
      },
      "source": [
        "def test1(generator, discriminator, num_epochs, metrics):\n",
        "    \"\"\"\n",
        "    Generates and saves a batch of fake images using the trained generator.\n",
        "\n",
        "    Args:\n",
        "        generator: Trained generator model.\n",
        "        discriminator: Trained discriminator model (unused here).\n",
        "        num_epochs: Number of epochs the model was trained for.\n",
        "        metrics: Dictionary containing training metrics (used for loss values).\n",
        "    \"\"\"\n",
        "    print('Testing Block.........')\n",
        "    # Get current Datetime\n",
        "    now = datetime.datetime.now()\n",
        "    # Extract latest Generator and discriminator losses\n",
        "    g_losses = metrics['train.G_losses'][-1]\n",
        "    d_losses = metrics['train.D_losses'][-1]\n",
        "\n",
        "\n",
        "    path='augGAN/output_images'\n",
        "    # Create Directory for saving output images if already doesn't exist\n",
        "    try:\n",
        "      os.mkdir(os.path.join('.', path))\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "\n",
        "    test_img_list = []\n",
        "    # Generate random noise input for the generator\n",
        "    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    # Generate fake images from the noise\n",
        "    test_fake = generator(test_noise).detach().cpu()\n",
        "    # Create grid of fake images for visualisaton\n",
        "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
        "\n",
        "    # Plotting the images\n",
        "    fig = plt.figure(figsize=(15,15))\n",
        "    fig = plt.axis(\"off\")\n",
        "    fig = plt.title(\"Fake Images\")\n",
        "    fig = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
        "    get_fig = plt.gcf()\n",
        "    fig = plt.show()\n",
        "    # Save the generated image with losses, epoch, and timestamp in filename\n",
        "    get_fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
        "                    (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjQZEYmqSRyn"
      },
      "source": [
        "def test2(generator, discriminator, num_epochs, metrics, loader):\n",
        "    \"\"\"\n",
        "    Generate and display real and fake images side by side, then save the figure.\n",
        "\n",
        "    Args:\n",
        "        generator: Trained generator model.\n",
        "        discriminator: Trained discriminator model (not used here).\n",
        "        num_epochs: Number of epochs the model was trained for.\n",
        "        metrics: Dictionary containing training metrics (used for loss values).\n",
        "        loader: DataLoader for real images.\n",
        "    \"\"\"\n",
        "    print('Testing Block.........')\n",
        "    now = datetime.datetime.now()\n",
        "\n",
        "    # Get the latest generator and Discriminator losses\n",
        "    g_losses = metrics['train.G_losses'][-1]\n",
        "    d_losses = metrics['train.D_losses'][-1]\n",
        "\n",
        "    # Create directory for saving output images if it already doesn't exist\n",
        "    path='augGAN/output_images'\n",
        "    try:\n",
        "      os.mkdir(os.path.join('.', path))\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "\n",
        "    # Get a batch of real images from the dataloader\n",
        "    real_batch = next(iter(loader))\n",
        "\n",
        "    test_img_list = []\n",
        "    # Generate random noise to feed the generator\n",
        "    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    # Generate fake images from the noise\n",
        "    test_fake = generator(test_noise).detach().cpu()\n",
        "    # Create a grid of fake images for display\n",
        "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
        "\n",
        "    fig = plt.figure(figsize=(15,15))\n",
        "\n",
        "    # Plot Real images\n",
        "    ax1 = plt.subplot(1,2,1)\n",
        "    ax1 = plt.axis(\"off\")\n",
        "    ax1 = plt.title(\"Real Images\")\n",
        "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "    # Plot fake images\n",
        "    ax2 = plt.subplot(1,2,2)\n",
        "    ax2 = plt.axis(\"off\")\n",
        "    ax2 = plt.title(\"Fake Images\")\n",
        "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
        "    #ax2 = plt.show()\n",
        "\n",
        "    # Save the figure with losses and epoch info in the filename\n",
        "    fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
        "                    (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncRL2jjtN-Ns"
      },
      "source": [
        "def test_fake(generator, discriminator, metrics, n_images, folname):\n",
        "    \"\"\"\n",
        "    Generates fake images using the generator, saves them in batches, and prints discriminator output stats.\n",
        "\n",
        "    Args:\n",
        "        generator: Trained generator model.\n",
        "        discriminator: Trained discriminator model.\n",
        "        metrics: Dictionary containing training metrics.\n",
        "        n_images: Total number of fake images to generate.\n",
        "        folname: Folder name for saving images.\n",
        "    \"\"\"\n",
        "\n",
        "    now = datetime.datetime.now()\n",
        "    g_losses = metrics['train.G_losses'][-1]\n",
        "    d_losses = metrics['train.D_losses'][-1]\n",
        "\n",
        "    # Define directory path for saving generated images\n",
        "    #path='augGAN/output_images/%+.3f_%+.3f_%d_%s'% (g_losses, d_losses, n_images, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "    path='main_folder/'+str(n_images)+'/'+folname\n",
        "    try:\n",
        "      os.mkdir(os.path.join('.', path))\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "\n",
        "    im_batch_size = 50    # Number of images to generate per batch\n",
        "    #n_images=100\n",
        "\n",
        "    # Generate images in batches to avoid memory overload\n",
        "    for i_batch in range(0, n_images, im_batch_size):\n",
        "        gen_z = torch.randn(im_batch_size, 100, 1, 1, device=device)\n",
        "        gen_images = generator(gen_z)\n",
        "\n",
        "        # Get discriminator's output on generated images\n",
        "        dis_result = discriminator(gen_images).view(-1)\n",
        "\n",
        "        # Prepare images for saving: move to CPU, detach from graph, convert to numpy, and transpose for saving\n",
        "        images = gen_images.to(\"cpu\").clone().detach()\n",
        "        images = images.numpy().transpose(0, 2, 3, 1)\n",
        "        # Save each generated image individually\n",
        "        for i_image in range(gen_images.size(0)):\n",
        "            save_image(gen_images[i_image, :, :, :], os.path.join(path,\n",
        "                        f'image_{i_batch+i_image:04d}.png'), normalize= True)\n",
        "\n",
        "    print('Testing Block.........')\n",
        "    # Print mean discriminator score for generated images\n",
        "    print('Discriminator_mean: ', dis_result.mean().item())\n",
        "\n",
        "    # Optional: archive saved images (commented out)\n",
        "    #import shutil\n",
        "    #shutil.make_archive('images', 'zip', './augGAN/output_images')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWeXmdInu38z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "944e507b-87ca-41fc-deb7-130a699dcbaf"
      },
      "source": [
        "# Create required directories if they don't already exist\n",
        "for func in [\n",
        "    lambda: os.mkdir(os.path.join('.', 'augGAN')),\n",
        "    lambda: os.mkdir(os.path.join('.', 'augGAN/model')),\n",
        "    lambda: os.mkdir(os.path.join('.', 'augGAN/plots')),\n",
        "    lambda: os.mkdir(os.path.join('.', 'augGAN/output_images'))]:  # create directories\n",
        "  try:\n",
        "    func()\n",
        "  except Exception as error:\n",
        "    print(error)  # Print error if directory already exists or other issues\n",
        "    continue\n",
        "\n",
        "# Define metric fields to keep track of during training\n",
        "METRIC_FIELDS = [\n",
        "    'train.D_x',\n",
        "    'train.D_G_z1',\n",
        "    'train.D_G_z2',\n",
        "    'train.G_losses',\n",
        "    'train.D_losses',\n",
        "]\n",
        "\n",
        "# Initialize metrics dictionary with empty lists for each metric\n",
        "metrics = {field: list() for field in METRIC_FIELDS}\n",
        "\n",
        "# Define normalization parameters and transforms depending on number of channels (nc)\n",
        "if nc==1:\n",
        "    mu = (0.5)\n",
        "    sigma = (0.5)\n",
        "    transform = transforms.Compose([\n",
        "        # Convert image to grayscale with 1 output channel\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        # Resize images to 64x64 pixels\n",
        "        transforms.Resize((64,64)),\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize pixel values to mean=0.5, std=0.5\n",
        "        transforms.Normalize(mu, sigma)])\n",
        "elif nc==3:\n",
        "    mu = (0.5,0.5,0.5)\n",
        "    sigma = (0.5,0.5,0.5)\n",
        "    # For RGB images: resize, convert to tensor, and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64,64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mu, sigma)])\n",
        "else:\n",
        "    print(\"Tranformation not defined for this option\")\n",
        "\n",
        "# Define the base data directory path\n",
        "data_dir = '/Thyroid Data DCGAN/DU'\n",
        "\n",
        "# Load different training datasets with the defined transform\n",
        "trainset0 = datasets.ImageFolder(os.path.join(data_dir, \"Train/\"), transform=transform)\n",
        "trainset500 = datasets.ImageFolder(os.path.join(data_dir, \"Train_classic/500/\"), transform=transform)\n",
        "trainset1000 = datasets.ImageFolder(os.path.join(data_dir, \"train_classic/1000/\"), transform=transform)\n",
        "trainset2000 = datasets.ImageFolder(os.path.join(data_dir, \"train_classic/2000/\"), transform=transform)\n",
        "\n",
        "# Combine datasets into lists for easier concatenation\n",
        "listtrainset_no_aug = [trainset0]\n",
        "listtrainset_classic = [trainset500, trainset1000]  # trainset2000 is commented out\n",
        "listtrainset = listtrainset_no_aug + listtrainset_classic\n",
        "\n",
        "# Concatenate all training datasets into a single dataset\n",
        "train_set = torch.utils.data.ConcatDataset(listtrainset)\n",
        "\n",
        "# Instantiate Generator and Discriminator models, move to device (CPU/GPU)\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Initialize model weights\n",
        "generator.apply(weights_init)\n",
        "discriminator.apply(weights_init)\n",
        "\n",
        "# Setup Adam optimizers with learning rates and beta parameters\n",
        "gen_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "dis_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
        "\n",
        "# Load pretrained models if flag is set\n",
        "if LOAD_MODEL:\n",
        "    if torch.cuda.is_available():\n",
        "        checkpoint = torch.load(PATH)\n",
        "    else:\n",
        "        # Load on CPU if CUDA not available\n",
        "        checkpoint = torch.load(PATH, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    # Load saved state dicts for generator, discriminator, and optimizers\n",
        "    generator.load_state_dict(checkpoint['state_dict_generator'])\n",
        "    discriminator.load_state_dict(checkpoint['state_dict_discriminator'])\n",
        "    gen_optimizer.load_state_dict(checkpoint['gen_optimizer'])\n",
        "    dis_optimizer.load_state_dict(checkpoint['dis_optimizer'])\n",
        "    metrics = checkpoint['metrics']\n",
        "    num_epochs = checkpoint['train_epoch']\n",
        "    date = checkpoint['date']\n",
        "\n",
        "    # Set models to evaluation mode (disable dropout, batchnorm updates)\n",
        "    generator.train(mode=False)\n",
        "    discriminator.train(mode=False)\n",
        "\n",
        "    print('GAN loaded for epochs: ', num_epochs)\n",
        "    print(generator)\n",
        "    print(discriminator)\n",
        "    print(gen_optimizer)\n",
        "    print(dis_optimizer)\n",
        "    print(date)\n",
        "    # Optionally run a test function here\n",
        "    # test1(generator, discriminator, num_epochs, metrics)\n",
        "\n",
        "else:\n",
        "    if TRAIN_ALL:\n",
        "        # Prepare dataloader for entire training set, shuffling enabled\n",
        "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Train GAN on full dataset\n",
        "        train_gan(generator, discriminator, gen_optimizer, dis_optimizer, train_loader,\n",
        "                  num_epochs, metrics)\n",
        "        # Run test function with trained models\n",
        "        test2(generator, discriminator, num_epochs, metrics, train_loader)\n",
        "    else:\n",
        "        # Optionally filter dataset by class index (commented example for x-ray dataset)\n",
        "        # idx = []\n",
        "        # idx = get_indices(train_set, 4, idx) # e.g., 0 for covid, etc.\n",
        "\n",
        "        # Create dataloader and mask to filter only samples with label 0 (e.g., covid)\n",
        "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "        mask = [x[1] == 0 for x in train_loader.dataset]  # select samples of class 0\n",
        "        idx = np.arange(len(train_loader.dataset))[mask]\n",
        "\n",
        "        print(\"Total samples now are \", len(idx))\n",
        "\n",
        "        # Create dataloader with subset sampler to load only filtered samples\n",
        "        selected_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                                      sampler=SubsetRandomSampler(idx))\n",
        "\n",
        "        # Train GAN on filtered subset\n",
        "        train_gan(generator, discriminator, gen_optimizer, dis_optimizer, selected_loader,\n",
        "                  num_epochs, metrics)\n",
        "        # Test on filtered subset\n",
        "        test2(generator, discriminator, num_epochs, metrics, selected_loader)\n",
        "        # Test on full set (or default)\n",
        "        test1(generator, discriminator, num_epochs, metrics)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name 'os' is not defined\n",
            "name 'os' is not defined\n",
            "name 'os' is not defined\n",
            "name 'os' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ea789ae4547c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Define normalization parameters and transforms depending on number of channels (nc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Zz7NErSjEG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "5044b9ed-2c23-4fd8-9fa1-6f495beb3f17"
      },
      "source": [
        "#Testing cell....to visualize\n",
        "test_batch = 1 #No of images to be genertaed in stack\n",
        "test_fake_id = 1 #To generate fake images or just view real images\n",
        "LOAD_ID = 0 #Class of images in case test_fake_id is 0\n",
        "\n",
        "if test_fake_id:\n",
        "  #check for fake image\n",
        "  test_img_list = []\n",
        "  test_noise = torch.randn(test_batch, nz, 1, 1, device=device)\n",
        "  test_img = generator(test_noise)#.detach().cpu()\n",
        "\n",
        "else:\n",
        "  #check for real image\n",
        "  idx = []\n",
        "  #train_set = datasets.ImageFolder(\"main_folder/100/\", transform=transform)\n",
        "  idx = get_indices(train_set, LOAD_ID, idx)\n",
        "  test_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch,\n",
        "                                                sampler = SubsetRandomSampler(idx))\n",
        "  data = next(iter(test_loader))\n",
        "  test_noise, test_class_lable = data\n",
        "  test_img.data.resize_(test_noise.size()).copy_(test_noise)\n",
        "  #print(data[0].size())\n",
        "  print('class label for real', test_class_lable)\n",
        "\n",
        "s_output = discriminator(test_img.detach().to(device))\n",
        "print('Discriminator s o/p', s_output)\n",
        "\n",
        "test_img = test_img.detach().cpu()\n",
        "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
        "plt.axis('off')\n",
        "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples of class 0 found are 60\n",
            "class label for real tensor([0])\n",
            "Discriminator s o/p tensor([[[[0.4945]]]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6058916b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpf-AsWkqzZf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df8ffdac-678a-42f9-b83f-cf06a500deed"
      },
      "source": [
        "test_fake(generator, discriminator, metrics, 500, \"normal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Block.........\n",
            "Discriminator_mean:  0.5166683793067932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_RDk-5_izjj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "671b61ee-3715-40ad-e821-c06b8a89ce00"
      },
      "source": [
        "#To confimr no of images in a specified class\n",
        "path, dirs, files = next(os.walk(os.path.join(data_dir, \"train/pneumonia_vir\")))\n",
        "print(len(files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8la1EFeB3zRF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c864d9ed-c889-4244-a5c4-db51946ca646"
      },
      "source": [
        "from torchsummary import summary\n",
        "# Generate architecture summary for generator and discriminator network\n",
        "\n",
        "summary(generator, (100, 1, 1))\n",
        "summary(discriminator, (1, 64, 64))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 1, 64, 64]           --\n",
            "|    └─ConvTranspose2d: 2-1              [-1, 512, 4, 4]           819,200\n",
            "|    └─BatchNorm2d: 2-2                  [-1, 512, 4, 4]           1,024\n",
            "|    └─ReLU: 2-3                         [-1, 512, 4, 4]           --\n",
            "|    └─ConvTranspose2d: 2-4              [-1, 256, 8, 8]           2,097,152\n",
            "|    └─BatchNorm2d: 2-5                  [-1, 256, 8, 8]           512\n",
            "|    └─ReLU: 2-6                         [-1, 256, 8, 8]           --\n",
            "|    └─ConvTranspose2d: 2-7              [-1, 128, 16, 16]         524,288\n",
            "|    └─BatchNorm2d: 2-8                  [-1, 128, 16, 16]         256\n",
            "|    └─ReLU: 2-9                         [-1, 128, 16, 16]         --\n",
            "|    └─ConvTranspose2d: 2-10             [-1, 64, 32, 32]          131,072\n",
            "|    └─BatchNorm2d: 2-11                 [-1, 64, 32, 32]          128\n",
            "|    └─ReLU: 2-12                        [-1, 64, 32, 32]          --\n",
            "|    └─ConvTranspose2d: 2-13             [-1, 1, 64, 64]           1,024\n",
            "|    └─Tanh: 2-14                        [-1, 1, 64, 64]           --\n",
            "==========================================================================================\n",
            "Total params: 3,574,656\n",
            "Trainable params: 3,574,656\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 423.53\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.91\n",
            "Params size (MB): 13.64\n",
            "Estimated Total Size (MB): 15.54\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 1, 1, 1]             --\n",
            "|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          1,024\n",
            "|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n",
            "|    └─Dropout: 2-3                      [-1, 64, 32, 32]          --\n",
            "|    └─Conv2d: 2-4                       [-1, 128, 16, 16]         131,072\n",
            "|    └─BatchNorm2d: 2-5                  [-1, 128, 16, 16]         256\n",
            "|    └─LeakyReLU: 2-6                    [-1, 128, 16, 16]         --\n",
            "|    └─Conv2d: 2-7                       [-1, 256, 8, 8]           524,288\n",
            "|    └─BatchNorm2d: 2-8                  [-1, 256, 8, 8]           512\n",
            "|    └─LeakyReLU: 2-9                    [-1, 256, 8, 8]           --\n",
            "|    └─Dropout: 2-10                     [-1, 256, 8, 8]           --\n",
            "|    └─Conv2d: 2-11                      [-1, 512, 4, 4]           2,097,152\n",
            "|    └─BatchNorm2d: 2-12                 [-1, 512, 4, 4]           1,024\n",
            "|    └─Dropout: 2-13                     [-1, 512, 4, 4]           --\n",
            "|    └─LeakyReLU: 2-14                   [-1, 512, 4, 4]           --\n",
            "|    └─Conv2d: 2-15                      [-1, 1, 1, 1]             8,192\n",
            "|    └─Sigmoid: 2-16                     [-1, 1, 1, 1]             --\n",
            "==========================================================================================\n",
            "Total params: 2,763,520\n",
            "Trainable params: 2,763,520\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 104.48\n",
            "==========================================================================================\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 1.38\n",
            "Params size (MB): 10.54\n",
            "Estimated Total Size (MB): 11.93\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Sequential: 1-1                        [-1, 1, 1, 1]             --\n",
              "|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          1,024\n",
              "|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n",
              "|    └─Dropout: 2-3                      [-1, 64, 32, 32]          --\n",
              "|    └─Conv2d: 2-4                       [-1, 128, 16, 16]         131,072\n",
              "|    └─BatchNorm2d: 2-5                  [-1, 128, 16, 16]         256\n",
              "|    └─LeakyReLU: 2-6                    [-1, 128, 16, 16]         --\n",
              "|    └─Conv2d: 2-7                       [-1, 256, 8, 8]           524,288\n",
              "|    └─BatchNorm2d: 2-8                  [-1, 256, 8, 8]           512\n",
              "|    └─LeakyReLU: 2-9                    [-1, 256, 8, 8]           --\n",
              "|    └─Dropout: 2-10                     [-1, 256, 8, 8]           --\n",
              "|    └─Conv2d: 2-11                      [-1, 512, 4, 4]           2,097,152\n",
              "|    └─BatchNorm2d: 2-12                 [-1, 512, 4, 4]           1,024\n",
              "|    └─Dropout: 2-13                     [-1, 512, 4, 4]           --\n",
              "|    └─LeakyReLU: 2-14                   [-1, 512, 4, 4]           --\n",
              "|    └─Conv2d: 2-15                      [-1, 1, 1, 1]             8,192\n",
              "|    └─Sigmoid: 2-16                     [-1, 1, 1, 1]             --\n",
              "==========================================================================================\n",
              "Total params: 2,763,520\n",
              "Trainable params: 2,763,520\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 104.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 1.38\n",
              "Params size (MB): 10.54\n",
              "Estimated Total Size (MB): 11.93\n",
              "=========================================================================================="
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N0yiqHfzOzX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9599816b-3707-49c0-c869-932427a16458"
      },
      "source": [
        "print(generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Z61n2SAhlv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "671b8da4-7a8b-4136-948f-9b057aa52075"
      },
      "source": [
        "print(discriminator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}